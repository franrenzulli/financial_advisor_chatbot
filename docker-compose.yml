version: '3.8'

services:
  frontend:
    build: ./frontend 
    ports:
      - "8501:8501" # Default Streamlit port
    depends_on:
      - backend
    environment:
      # The backend service will be reachable at http://backend:8000 from within the Docker network
      BACKEND_URL: http://backend:8000

  backend:
    build: ./backend 
    ports:
      - "8000:8000" # FastAPI default port
    environment:
      # If your backend needs to connect to ChromaDB, specify its internal network address
      CHROMADB_HOST: vector_db
      CHROMADB_PORT: 8000 # Default ChromaDB port (check if this is correct for your ChromaDB setup)
      # You might also need OpenAI API key here, or manage it securely elsewhere
      OPENAI_API_KEY: ${OPENAI_API_KEY} # Best practice: use environment variable

  ingestion:
    build:
      context: .
      dockerfile: backend/llm/Dockerfile
    container_name: ingestion-job
    env_file: .env
    volumes:
      - ./backend/llm/vector-store:/app/vector-store
      - ./backend/llm/ingestion:/app  # Cambia esto!
    working_dir: /app
    command: ["python", "process_s3_documents.py"]
